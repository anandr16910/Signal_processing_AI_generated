#!/usr/bin/env python3
"""
GitHub incident collector for "Parseval Paradox" / energy drift bugs.

Searches public GitHub issues for MATLAB-related FFT/IFFT scaling,
energy mismatch, Parseval violations, etc., and writes them to
data/energy_drift_incidents.csv
"""

import os
import csv
import time
import requests
from datetime import datetime, timedelta

GITHUB_API_URL = "https://api.github.com/search/issues"
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")  # Actions / local env

# --- CONFIG -------------------------------------------------------------

PHYSICS_KEYWORDS = [
    "parseval",
    "\"parseval's theorem\"",
    "energy drift",
    "energy leak",
    "\"signal energy\"",
    "\"total energy\"",
    "\"power mismatch\"",
    "\"fft scaling\"",
    "\"ifft scaling\"",
    "\"fft normalization\"",
    "\"ifft normalization\"",
    "\"unitary fft\"",
    "\"1/sqrt(N)\"",
    "\"sqrt(N)\"",
]

AI_KEYWORDS = [
    "chatgpt",
    "copilot",
    "llm",
    "ai generated",
    "ai-generated",
    "generated by ai",
]

DAYS_BACK = 365  # search last year
OUTPUT_CSV = "data/energy_drift_incidents.csv"

# ------------------------------------------------------------------------


def build_queries():
    date_from = (datetime.utcnow() - timedelta(days=DAYS_BACK)).strftime("%Y-%m-%d")
    base_filters = [
        "is:issue",
        "in:title,body",
        "language:Matlab",
        f"created:>={date_from}",
    ]

    queries = []

    # Physics-only queries
    for kw in PHYSICS_KEYWORDS:
        queries.append(" ".join(base_filters + [kw]))

    # Physics + AI
    for p in PHYSICS_KEYWORDS:
        for a in AI_KEYWORDS:
            queries.append(" ".join(base_filters + [p, a]))

    # de-duplicate while preserving order
    seen = set()
    unique = []
    for q in queries:
        if q not in seen:
            seen.add(q)
            unique.append(q)
    return unique


def github_search(query, per_page=50, max_pages=5):
    headers = {
        "Accept": "application/vnd.github+json",
        "X-GitHub-Api-Version": "2022-11-28",
        "User-Agent": "parseval-paradox-research-bot",
    }
    if GITHUB_TOKEN:
        headers["Authorization"] = f"Bearer {GITHUB_TOKEN}"

    for page in range(1, max_pages + 1):
        params = {"q": query, "per_page": per_page, "page": page}
        resp = requests.get(GITHUB_API_URL, headers=headers, params=params)
        if resp.status_code != 200:
            print(f"[WARN] {resp.status_code} for query: {query}")
            print(resp.text[:200])
            break

        data = resp.json()
        items = data.get("items", [])
        if not items:
            break

        for item in items:
            yield item

        # simple rate-limit friendliness
        time.sleep(2)


def classify_incident(body_text: str) -> str:
    text = (body_text or "").lower()
    if any(k in text for k in (k.lower() for k in AI_KEYWORDS)):
        return "AI-related"
    return "Unknown/Not mentioned"


def ensure_dir_for(path):
    os.makedirs(os.path.dirname(path), exist_ok=True)


def main():
    ensure_dir_for(OUTPUT_CSV)
    queries = build_queries()
    print(f"Running {len(queries)} search queries")

    seen_ids = set()
    rows = []

    for q in queries:
        print(f"\n[QUERY] {q}")
        for issue in github_search(q):
            if issue["id"] in seen_ids:
                continue
            seen_ids.add(issue["id"])

            body = issue.get("body") or ""
            snippet = body.replace("\n", " ")[:280]

            repo_full = issue["repository_url"].split("repos/")[-1]

            rows.append({
                "issue_id": issue["id"],
                "repo": repo_full,
                "number": issue["number"],
                "title": issue["title"],
                "url": issue["html_url"],
                "state": issue["state"],
                "created_at": issue["created_at"],
                "updated_at": issue["updated_at"],
                "score": issue.get("score", ""),
                "classification": classify_incident(body),
                "snippet": snippet,
                "query_used": q,
            })

    fieldnames = [
        "issue_id", "repo", "number", "title", "url",
        "state", "created_at", "updated_at", "score",
        "classification", "snippet", "query_used",
    ]

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    print(f"\nSaved {len(rows)} issues to {OUTPUT_CSV}")


if __name__ == "__main__":
    main()
